# LLaMA Efficient Tuning

[![GitHub Code License](https://img.shields.io/github/license/hiyouga/LLaMA-Efficient-Tuning)](LICENSE)
[![GitHub last commit](https://img.shields.io/github/last-commit/hiyouga/LLaMA-Efficient-Tuning)](https://github.com/hiyouga/LLaMA-Efficient-Tuning/commits/main)

ğŸ‘‹ åŠ æˆ‘çš„[å¾®ä¿¡](assets/wechat.jpg)ã€‚

## æ›´æ–°æ—¥å¿—

[23/09/28]
æ”¯æŒbaichuanã€llama2ã€llamaã€glm2ç­‰å¤§æ¨¡å‹ï¼Œæ”¯æŒQLoRAï¼›
æ”¯æŒgpué¢„è§ˆã€å¤§æ¨¡å‹å¾®è°ƒè®­ç»ƒã€æ¨¡å‹åˆå¹¶ã€éƒ¨ç½²æœåŠ¡ï¼ˆæµ‹è¯•ä¸­ï¼‰ï¼›
æ”¯æŒapiæ–¹å¼ï¼Œå¾®è°ƒè®­ç»ƒã€æ¨¡å‹åˆå¹¶ã€æ¨¡å‹å‘å¸ƒï¼›
å¢åŠ æ•°æ®ç®¡ç†ï¼Œæ”¯æŒä¸Šä¼ excelæ–‡ä»¶ï¼›
å¢åŠ è®­ç»ƒè„šæœ¬ç®¡ç†ï¼Œæ”¯æŒè‡ªå®šä¹‰è„šæœ¬ç¼–è¾‘å’Œè®­ç»ƒï¼Œæ”¯æŒpt/sft/rm/ppo/dpoè®­ç»ƒé˜¶æ®µï¼Œæ”¯æŒtrain/eval/predictä»»åŠ¡ï¼›
å¢åŠ æ¨¡å‹å¿«é€Ÿç¼–è¾‘èƒ½åŠ›ï¼Œèƒ½å¤Ÿç¼–è¾‘æŒ‡å®šé—®é¢˜çš„å›ç­”ï¼›


## æ¨¡å‹

| æ¨¡å‹å                                                   | æ¨¡å‹å¤§å°                     | é»˜è®¤æ¨¡å—           | Template  |
| -------------------------------------------------------- | --------------------------- | ----------------- | --------- |
| [LLaMA](https://github.com/facebookresearch/llama)       | 7B/13B/33B/65B              | q_proj,v_proj     | -         |
| [LLaMA-2](https://huggingface.co/meta-llama)             | 7B/13B/70B                  | q_proj,v_proj     | llama2    |
| [BLOOM](https://huggingface.co/bigscience/bloom)         | 560M/1.1B/1.7B/3B/7.1B/176B | query_key_value   | -         |
| [BLOOMZ](https://huggingface.co/bigscience/bloomz)       | 560M/1.1B/1.7B/3B/7.1B/176B | query_key_value   | -         |
| [Falcon](https://huggingface.co/tiiuae/falcon-7b)        | 7B/40B                      | query_key_value   | -         |
| [Baichuan](https://github.com/baichuan-inc/Baichuan-13B) | 7B/13B                      | W_pack            | baichuan  |
| [Baichuan2](https://github.com/baichuan-inc/Baichuan2)   | 7B/13B                      | W_pack            | baichuan2 |
| [InternLM](https://github.com/InternLM/InternLM)         | 7B/20B                      | q_proj,v_proj     | intern    |
| [Qwen](https://github.com/QwenLM/Qwen-7B)                | 7B/14B                      | c_attn            | chatml    |
| [XVERSE](https://github.com/xverse-ai/XVERSE-13B)        | 13B                         | q_proj,v_proj     | xverse    |
| [ChatGLM2](https://github.com/THUDM/ChatGLM2-6B)         | 6B                          | query_key_value   | chatglm2  |
| [Phi-1.5](https://huggingface.co/microsoft/phi-1_5)      | 1.3B                        | Wqkv              | -         |



## è½¯ä»¶ä¾èµ–

- Python 3.10 å’Œ PyTorch 1.13.1
- ğŸ¤—Transformers, Datasets, Accelerate, PEFT å’Œ TRL
- sentencepiece, protobuf å’Œ tiktoken
- jieba, rouge-chinese å’Œ nltk (ç”¨äºè¯„ä¼°)
- gradio å’Œ matplotlib (ç”¨äºç½‘é¡µç«¯äº¤äº’)
- uvicorn, fastapi å’Œ sse-starlette (ç”¨äº API)


## ç›®å½•è¯´æ˜
- run_server.py              # å¯åŠ¨ç¨‹åº
- main.py              # FastAPIåº”ç”¨ç¨‹åº
- apis/                 # APIæ¥å£
- config/                 # é…ç½®æ–‡ä»¶
- docs/                 # è¯´æ˜æ–‡æ¡£
- logs/                # æ—¥å¿—ç›®å½•
- peft-xxx/              # peft
- scripts/             # è®­ç»ƒç­‰è„šæœ¬
- templates/              # htmlæ¨¡æ¿
- test/                # æµ‹è¯•ç”¨ä¾‹



## é¡¹ç›®å¯åŠ¨
> python run_server.py

## docker
### æ„å»ºé•œåƒ
> docker build -t docker.li.com/llm_finetune .
### è¿è¡Œå®¹å™¨
> docker run --gpus all --network host --ipc host --name llm_finetune -d \
--ulimit memlock=-1 --ulimit stack=67108864 \
-v /data0/service/llm_finetune/config/171.env.yaml:/app/config/env.yaml \
-v /var/run/docker.sock:/var/run/docker.sock \
-v /data0/LLMs:/app/original-weights \
-v /data0/service/llm_finetune/logs:/app/logs \
-v /data0/service/llm_finetune/data:/app/data \
--security-opt seccomp=unconfined \
docker.li.com/llm_finetune

## æœåŠ¡éªŒè¯
### webæœåŠ¡
> http://localhost:8088/docs

### é¦–é¡µ
> http://localhost:8088

### ç¦»çº¿é¡¹ç›®ç¯å¢ƒ
```shell
docker save docker.li.com/llm_finetune -o ./llm_finetune.img
tar -czvf llm_finetune.img.tgz llm_finetune.img

tar -zxvf llm_finetune.img.tgz
docker load --input llm_finetune.img
docker images
```


### æµè§ˆå™¨æµ‹è¯•

```bash
python src/web_demo.py \
    --model_name_or_path path_to_llama_model \
    --template default \
    --finetuning_type lora \
    --checkpoint_dir path_to_checkpoint
```



## åè®®

æœ¬ä»“åº“çš„ä»£ç ä¾ç…§ [Apache-2.0](LICENSE) åè®®å¼€æºã€‚

ä½¿ç”¨æ¨¡å‹æƒé‡æ—¶ï¼Œè¯·éµå¾ªå¯¹åº”çš„æ¨¡å‹åè®®ï¼š

- [LLaMA](https://github.com/facebookresearch/llama/blob/main/MODEL_CARD.md)
- [LLaMA-2](https://ai.meta.com/llama/license/)
- [BLOOM](https://huggingface.co/spaces/bigscience/license)
- [Falcon](LICENSE)
- [Baichuan](https://huggingface.co/baichuan-inc/baichuan-7B/resolve/main/baichuan-7B%20%E6%A8%A1%E5%9E%8B%E8%AE%B8%E5%8F%AF%E5%8D%8F%E8%AE%AE.pdf)
- [Baichuan2](https://huggingface.co/baichuan-inc/Baichuan2-7B-Base/resolve/main/Baichuan%202%E6%A8%A1%E5%9E%8B%E7%A4%BE%E5%8C%BA%E8%AE%B8%E5%8F%AF%E5%8D%8F%E8%AE%AE.pdf)
- [InternLM](https://github.com/InternLM/InternLM#open-source-license)
- [Qwen](https://huggingface.co/Qwen/Qwen-7B-Chat/blob/main/LICENSE)
- [XVERSE](https://github.com/xverse-ai/XVERSE-13B/blob/main/MODEL_LICENSE.pdf)
- [ChatGLM2](https://github.com/THUDM/ChatGLM2-6B/blob/main/MODEL_LICENSE)
- [Phi-1.5](https://huggingface.co/microsoft/phi-1_5/resolve/main/Research%20License.docx)

## å¼•ç”¨

å¦‚æœæ‚¨è§‰å¾—æ­¤é¡¹ç›®æœ‰å¸®åŠ©ï¼Œè¯·è€ƒè™‘ä»¥ä¸‹åˆ—æ ¼å¼å¼•ç”¨

```bibtex
@Misc{llama-efficient-tuning,
  title = {LLaMA Efficient Tuning},
  author = {hiyouga},
  howpublished = {\url{https://github.com/hiyouga/LLaMA-Efficient-Tuning}},
  year = {2023}
}
```

## è‡´è°¢

æœ¬é¡¹ç›®å—ç›Šäº [LLaMA-Efficient-Tuning](https://github.com/hiyouga/LLaMA-Efficient-Tuning)ï¼Œæ„Ÿè°¢ä½œè€…çš„ä»˜å‡ºã€‚

